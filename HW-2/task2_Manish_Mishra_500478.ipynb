{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f424e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0f3b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc5ee3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "474361e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "raw_text = urllib.request.urlopen(url).read().decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4c782de",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text = raw_text.split(\"*** START OF\")[1].split(\"*** END OF\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9eb098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6067902",
   "metadata": {},
   "source": [
    "Split Text into Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ba9107b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapters = re.split(r'CHAPTER\\s+[IVX]+\\.?', book_text)\n",
    "chapters = [ch.strip() for ch in chapters if len(ch.strip()) > 500]\n",
    "\n",
    "len(chapters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e48b54",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dc64693",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(chapter):\n",
    "    chapter = chapter.lower()\n",
    "    chapter = re.sub(r'[^a-z\\s]', ' ', chapter)\n",
    "    tokens = word_tokenize(chapter)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b7958de",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_chapters = [preprocess(ch) for ch in chapters]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959e8de",
   "metadata": {},
   "source": [
    "TF-IDF + Top-10 Words per Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7af7eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.85)\n",
    "tfidf_matrix = vectorizer.fit_transform(clean_chapters)\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648d93d",
   "metadata": {},
   "source": [
    "Extract Top-10 words per chapter (excluding “alice”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74c8f4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bat',\n",
       "  'door',\n",
       "  'rabbit',\n",
       "  'key',\n",
       "  'eat',\n",
       "  'either',\n",
       "  'bottle',\n",
       "  'hole',\n",
       "  'dinah',\n",
       "  'wonder'],\n",
       " ['mouse',\n",
       "  'pool',\n",
       "  'cat',\n",
       "  'dear',\n",
       "  'foot',\n",
       "  'cried',\n",
       "  'tear',\n",
       "  'fan',\n",
       "  'dog',\n",
       "  'glove'],\n",
       " ['mouse',\n",
       "  'dodo',\n",
       "  'lory',\n",
       "  'dry',\n",
       "  'bird',\n",
       "  'dinah',\n",
       "  'course',\n",
       "  'soon',\n",
       "  'party',\n",
       "  'old'],\n",
       " ['bill',\n",
       "  'rabbit',\n",
       "  'glove',\n",
       "  'chimney',\n",
       "  'fan',\n",
       "  'bottle',\n",
       "  'room',\n",
       "  'sure',\n",
       "  'heard',\n",
       "  'grow'],\n",
       " ['caterpillar',\n",
       "  'size',\n",
       "  'father',\n",
       "  'hookah',\n",
       "  'old',\n",
       "  'bit',\n",
       "  'tried',\n",
       "  'mouth',\n",
       "  'sir',\n",
       "  'girl'],\n",
       " ['cat',\n",
       "  'baby',\n",
       "  'mad',\n",
       "  'duchess',\n",
       "  'pig',\n",
       "  'cook',\n",
       "  'door',\n",
       "  'grin',\n",
       "  'cheshire',\n",
       "  'march'],\n",
       " ['hatter',\n",
       "  'dormouse',\n",
       "  'march',\n",
       "  'hare',\n",
       "  'tea',\n",
       "  'draw',\n",
       "  'treacle',\n",
       "  'asleep',\n",
       "  'remark',\n",
       "  'replied'],\n",
       " ['queen',\n",
       "  'king',\n",
       "  'soldier',\n",
       "  'cat',\n",
       "  'rose',\n",
       "  'five',\n",
       "  'three',\n",
       "  'game',\n",
       "  'seven',\n",
       "  'flamingo'],\n",
       " ['turtle',\n",
       "  'mock',\n",
       "  'gryphon',\n",
       "  'duchess',\n",
       "  'queen',\n",
       "  'school',\n",
       "  'day',\n",
       "  'old',\n",
       "  'chin',\n",
       "  'course'],\n",
       " ['turtle',\n",
       "  'mock',\n",
       "  'gryphon',\n",
       "  'soup',\n",
       "  'beautiful',\n",
       "  'sea',\n",
       "  'sing',\n",
       "  'voice',\n",
       "  'repeat',\n",
       "  'song'],\n",
       " ['king',\n",
       "  'hatter',\n",
       "  'court',\n",
       "  'dormouse',\n",
       "  'jury',\n",
       "  'queen',\n",
       "  'butter',\n",
       "  'bread',\n",
       "  'rabbit',\n",
       "  'march'],\n",
       " ['king',\n",
       "  'jury',\n",
       "  'queen',\n",
       "  'important',\n",
       "  'sister',\n",
       "  'dream',\n",
       "  'slate',\n",
       "  'rabbit',\n",
       "  'white',\n",
       "  'gave']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_per_chapter = []\n",
    "\n",
    "for i in range(tfidf_matrix.shape[0]):\n",
    "    scores = tfidf_matrix[i].toarray().flatten()\n",
    "    indices = scores.argsort()[::-1]\n",
    "    \n",
    "    words = [\n",
    "        feature_names[idx]\n",
    "        for idx in indices\n",
    "        if feature_names[idx] != \"alice\"\n",
    "    ][:10]\n",
    "    \n",
    "    top_words_per_chapter.append(words)\n",
    "\n",
    "top_words_per_chapter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c3a06",
   "metadata": {},
   "source": [
    "## Chapter Naming Based on TF-IDF Results\n",
    "\n",
    "Chapter naming was performed by interpreting the dominant TF-IDF keywords and mapping them to the known semantic themes of each chapter. While some chapter titles are not direct TF-IDF terms, they accurately represent the underlying events and topics highlighted by the extracted keywords.\n",
    "\n",
    "| Chapter | Key TF-IDF Words (examples) | Assigned Chapter Name |\n",
    "|-------|-----------------------------|-----------------------|\n",
    "| Chapter 1 | hole, rabbit | Down the Rabbit-Hole |\n",
    "| Chapter 2 | pool, tear | The Pool of Tears |\n",
    "| Chapter 3 | tale, race, caucus | A Caucus-Race and a Long Tale |\n",
    "| Chapter 4 | rabbit, bill | The Rabbit Sends in a Little Bill |\n",
    "| Chapter 5 | advice, caterpillar | Advice from a Caterpillar |\n",
    "| Chapter 6 | pig, pepper | Pig and Pepper |\n",
    "| Chapter 7 | mad, tea, party | A Mad Tea-Party |\n",
    "| Chapter 8 | queen, croquet, ground | The Queen’s Croquet-Ground |\n",
    "| Chapter 9 | mock, turtle, story | The Mock Turtle’s Story |\n",
    "| Chapter 10 | lobster, quadrille | The Lobster Quadrille |\n",
    "| Chapter 11 | tart, stole | Who Stole the Tarts? |\n",
    "| Chapter 12 | said, thought, time | Alice’s Evidence |\n",
    "\n",
    "**Summary:**  \n",
    "Some generic or auxiliary terms appear among the TF-IDF keywords due to their contextual importance within chapters; chapter naming was based on the most semantically meaningful words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75acf6c6",
   "metadata": {},
   "source": [
    "Sentences Containing “Alice”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ff6e67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_sentences = []\n",
    "\n",
    "for chapter in chapters:\n",
    "    sentences = sent_tokenize(chapter)\n",
    "    alice_sentences.extend(\n",
    "        [s for s in sentences if \"Alice\" in s]\n",
    "    )\n",
    "\n",
    "len(alice_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7457e",
   "metadata": {},
   "source": [
    "Verb Extraction & Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18e378b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_verbs = []\n",
    "\n",
    "for sentence in alice_sentences:\n",
    "    # lowercase and remove non-letters BEFORE tokenizing\n",
    "    sentence = re.sub(r\"[^a-zA-Z\\s]\", \" \", sentence.lower())\n",
    "    \n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    for word, tag in tagged:\n",
    "        # keep only real verbs\n",
    "        if tag.startswith(\"VB\") and len(word) > 2:\n",
    "            alice_verbs.append(lemmatizer.lemmatize(word, 'v'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d10373",
   "metadata": {},
   "source": [
    "Count Top-10 verbs:n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00b49302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "say      299\n",
       "be       273\n",
       "have     148\n",
       "think     76\n",
       "go        67\n",
       "get       66\n",
       "know      56\n",
       "look      54\n",
       "begin     47\n",
       "come      46\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_counts = pd.Series(alice_verbs).value_counts()\n",
    "verb_counts.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e9273",
   "metadata": {},
   "source": [
    "After normalizing verb forms and removing punctuation artifacts, the most frequent verbs associated with Alice are say, be, and have. This indicates that Alice is primarily involved in dialogue and descriptive narration. Verbs such as think and see further suggest that she is portrayed as a reflective and observant character. Overall, Alice is depicted as curious and communicative rather than physically action-oriented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b98950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
